\documentclass[12pt,a4paper,titlepage]{article}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{nccmath}
\usepackage{mathtools}
\usepackage{array}
\usepackage{fixmath}
\usepackage{mathrsfs}

\setlength{\textwidth}{15cm} \setcounter{page}{159}

\begin{document}


\begin{titlepage}
    \begin{center}
        \vspace*{3.5cm}
        
        \textbf{\huge{Forschungspraxis Proposal}}
        
        \vspace{8cm}

        \begin{verse}
            \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textbf{\large{Student: Zhiwei Han}}\\
            \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textbf{\large{Betreuer1: Hao Shen}}\\
            \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textbf{\large{Betreuer2: Dominik Meyer}}\\
        \end{verse}


        
        \vspace{1cm}
        
%        \includegraphics[width=0.4\textwidth]{university} 
        
        Lehrstuhl für Datenverarbeitung\\
        Technische Universtät München\\
        \today
        
    \end{center}
\end{titlepage}



\setlength{\parindent}{0pt} \setlength{\parskip}{2ex plus 0.5ex
minus 0.2ex}


\section*{1. Titel}
\subsection*{1.1 Topic Description}
In this practical research, an efficient online TD learning algorithm will be implmented with the help of sparsity, generated by $\ell1$ regularization and kernel method will also be used to aim at a better representation of features in high dimensional space.
\subsection*{1.2 Rough Titel}
The Implementation of An Online Selective kernel-based TD Learning with $\ell1$ regularization
\section*{2. Motivation}
The main purpose of this practical research is tring to solve two main problems in reinforcement learning,\\

1. The performance of reinforcement learning algorithms depends heavily on the feature selecting of value function approximation. Even the most expericend experts can struggle with this task, therefor a proper resentation methlogy is badly needed. 

2. In traditional reinforcement learning algorithm, dimension disaster stays always as a limitation, in large scale application like GO, the computational complexity grow exponentially. Hence the sparsity could be a proper key in computational expanse and memory consumption reduction.

\section*{3. Method, Goal and Verification}
The goal of this practical research is developing an more efficient online TD learning than traditional TD algorithm from the perspective of running time and memory consumption.\\
Although feature selection is usually limited by the understanding to problem, an appropriate expression of state can still be derivated by using some specific kernel functions. The reason is that, kernel method has the ability to turn trivial features into high dimensional space. Therefore the new features will has a much richer representation of problems.\\
Currently many studies have concentrated on reducing the computational expanse of large scale or complex problems and one very efficient away is to take advantage of the sparsity generated by $\ell1$ regularization. With that, the performance of learning algorithm will be will be greatly improved while much dirty computational work can be reduced.

The efficiency of developed algorithm will through comparison with standard

\section*{4. Project organisation}

\end{document}
